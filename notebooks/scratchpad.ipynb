{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nicho\\PyProjects\\CGAEL\\.conda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ischar(token):\n",
    "    \"\"\"\n",
    "    Returns true only if provided 'token' is a string with a length of exactly 1.\n",
    "    \"\"\"\n",
    "    if not isinstance(token, str):\n",
    "        return False\n",
    "    return len(token) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_len_trim_to(data:list, length:int):\n",
    "    \"\"\"\n",
    "    If a list is longer than the provided length, it will be returned trimmed to the provided length.\n",
    "    If the list is shorter or equal in length, it will be returned unchanged.\n",
    "    \"\"\"\n",
    "    return data[:min(len(data), length)]\n",
    "\n",
    "def list_len_pad_to(data:list, length:int, pad):\n",
    "    \"\"\"\n",
    "    If a list is shorter than the provided length, it will be returned padded to the provided length by the pad token.\n",
    "    If the list is longer or equal in length, it will be returned unchanged.\n",
    "    \"\"\"\n",
    "    return data + [pad] * (length - len(data))\n",
    "\n",
    "def list_len_force_to(data:list, length:int, pad):\n",
    "    \"\"\"\n",
    "    If a list is longer than the provided length, it will be returned trimmed to the provided length.\n",
    "    If a list is shorter than the provided length, it will be returned padded to the provided length by the pad token.\n",
    "    If the list is equal in length, it will be returned unchanged.\n",
    "    \"\"\"\n",
    "    data = list_len_trim_to(data, length)\n",
    "    return list_len_pad_to(data, length, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageTokenSet():\n",
    "    def __init__(self, alphabet_tokens:list, pad_token:str):\n",
    "        # Ensure the pad_token is a single character.\n",
    "        if not ischar(pad_token):\n",
    "            raise ValueError(f\"'pad_token' must be a single character string. Got {pad_token}.\")\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "        # Get the alphabet_tokens as both a list and a set.\n",
    "        # The list will maintain the order of the tokens. (Later stored.)\n",
    "        # The set will allow for fast error checking. (Discarded after.)\n",
    "        if isinstance(alphabet_tokens, list):\n",
    "            alph_list = [x for x in alphabet_tokens] # deepcopy\n",
    "            alph_set = set(alphabet_tokens)\n",
    "        elif isinstance(alphabet_tokens, set):\n",
    "            alph_list = list(alphabet_tokens)\n",
    "            alph_set = alphabet_tokens\n",
    "        elif isinstance(alphabet_tokens, str):\n",
    "            alph_list = [*alphabet_tokens]\n",
    "            alph_set = set(alph_list)\n",
    "        else:\n",
    "            raise ValueError(f\"'alphabet_tokens' should be provided as either a list, set or string. Got type '{type(alphabet_tokens)}'.\")\n",
    "        \n",
    "        # Check for errors in alphabet.\n",
    "\n",
    "        # List and set size should be the same. If different, this indicates a duplicate token.\n",
    "        if len(alph_list) != len(alph_set):\n",
    "            raise ValueError(f\"{len(alph_list)-len(alph_set)} duplicate tokens detected in 'alphabet_tokens'.\")\n",
    "        # Pad token cannot be in alphabet.\n",
    "        if self.pad_token in alph_set:\n",
    "            raise ValueError(f\"'pad_token' {self.pad_token} cannot be in alphabet.\")\n",
    "        # Check that each token is a single character.\n",
    "        for x in alph_list:\n",
    "            if not ischar(x):\n",
    "                raise ValueError(f\"Each entry of 'alphabet_tokens' must be a single character. Got '{x}'.\")\n",
    "        \n",
    "        # Alphabet tokens validated. Store.\n",
    "        self.alphabet_tokens = alph_list\n",
    "\n",
    "        # Instantiate encoder & decoder.\n",
    "        self._encoder = layer.StringLookup(vocabulary=self.alphabet_tokens, oov_token=self.pad_token, output_mode=\"int\", invert=False)\n",
    "        self._decoder = layer.StringLookup(vocabulary=self.alphabet_tokens, oov_token=self.pad_token, output_mode=\"int\", invert=True)\n",
    "\n",
    "    @property\n",
    "    def token_count(self):\n",
    "        \"\"\"\n",
    "        The number of tokens in the language, including the pad token.\n",
    "        \"\"\"\n",
    "        return self.encoder.vocabulary_size()\n",
    "    \n",
    "    def encode(self, data:str, shape:tuple):\n",
    "        \"\"\"\n",
    "        Encodes a Python string into a TensorFlow tensor.\n",
    "        \"\"\"\n",
    "        # Different rank shapes need to be handled differently.\n",
    "        if len(shape) == 1:\n",
    "            # Just need to ensure that the resulting list is of the proper size.\n",
    "            data = list_len_force_to([*data], shape[0], self.pad_token)\n",
    "        elif len(shape) == 2:\n",
    "            # Split the input by whitespace.\n",
    "            data = [list(x) for x in data.split()]\n",
    "            # Ensure each word is the proper length.\n",
    "            data = [list_len_force_to(x, shape[1], self.pad_token) for x in data]\n",
    "            # Ensure there are the proper number of \"words\".\n",
    "            data = list_len_force_to(data, shape[0], [self.pad_token]*shape[1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported shape rank={len(shape)}.\")\n",
    "        # Send to encoder.\n",
    "        return self._encoder(data)\n",
    "    \n",
    "    def decode(self, data):\n",
    "        \"\"\"\n",
    "        Encodes a TensorFlow tensor, NumPy array or Python array to a Python string.\n",
    "        \"\"\"\n",
    "        # Send to decoder.\n",
    "        data = self._decoder(data).numpy()\n",
    "        # Different rank shapes need to be handled differently.\n",
    "        if len(data.shape) == 1:\n",
    "            data = b''.join(data).decode(\"utf-8\").rstrip(self.pad_token)\n",
    "        elif len(data.shape) == 2:\n",
    "            # Join letters.\n",
    "            data = [b''.join(x).decode(\"utf-8\").rstrip(self.pad_token) for x in data]\n",
    "            # Join words.\n",
    "            data = ' '.join([x for x in data if len(x) > 0])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported shape rank={len(data.shape)}.\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 3 4 1 2]\n",
      " [4 2 3 4 0]\n",
      " [1 3 4 0 0]], shape=(3, 5), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CATCH THAT CAT'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = LanguageTokenSet(\"CHAT\", '-')\n",
    "a = chat.encode(\"CATCH THAT CAT\", shape=(3,5))\n",
    "print(a)\n",
    "chat.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4 3 0 2 3]\n",
      " [0 4 3 1 3]\n",
      " [3 4 0 0 0]], shape=(3, 5), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[1 1 0 1 1]\n",
      " [0 1 1 1 1]\n",
      " [1 1 0 0 0]], shape=(3, 5), dtype=int64)\n",
      "tf.Tensor([1 0 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([1 0 0], shape=(3,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[1 1 0 1 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]], shape=(3, 5), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[1 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]], shape=(3, 5), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[4 3 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]], shape=(3, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "data = chat._encoder([[*\"TA-HA\"], [*\"-TACA\"], [*\"AT---\"]])\n",
    "print(data)\n",
    "\n",
    "# Create a binary mask for the data where:\n",
    "# - 0 represents pad tokens.\n",
    "# - 1 represents any other token.\n",
    "# Pad tokens are always represented by 0 and all other tokens are positive so:\n",
    "mask = tf.math.sign(data)\n",
    "print(mask)\n",
    "\n",
    "# Apply column-wise denoising.\n",
    "\n",
    "# Gather the starting values of each row of the mask.\n",
    "# This will highlight all rows starting with pad tokens.\n",
    "col = mask[:, 0]\n",
    "print(col)\n",
    "# Apply the cumulative product to the isolated column.\n",
    "# Since 1*1=1, 1*0=0, 0*0=0, the column is left with a sequence of 1's followed only by 0's.\n",
    "# This will allow us to remove all values after first empty word.\n",
    "col = tf.math.cumprod(col, axis=-1)\n",
    "print(col)\n",
    "# Multiply this isolated column back over the mask.\n",
    "mask = tf.multiply(mask, col[..., tf.newaxis])\n",
    "print(mask)\n",
    "\n",
    "# Apply row-wise denoising.\n",
    "# Use cumulative product for the same reasoning as in column-wise denoising.\n",
    "mask = tf.math.cumprod(mask, axis=-1)\n",
    "print(mask)\n",
    "\n",
    "# Multiply the (binary) mask back over the data.\n",
    "result = tf.math.multiply(data, mask)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4 3 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]], shape=(3, 5), dtype=int64)\n",
      "tf.Tensor([4 3 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def denoise_language(data):\n",
    "    \"\"\"\n",
    "    Denoises language data when it is represented as a tensor of integers.\n",
    "    \"\"\"\n",
    "    # Create a binary mask for the data where:\n",
    "    # - 0 represents pad tokens.\n",
    "    # - 1 represents any other token.\n",
    "    # Pad tokens are always represented by 0 and all other tokens are positive so:\n",
    "    mask = tf.math.sign(data)\n",
    "\n",
    "    # Apply column-wise denoising.\n",
    "    if len(data.shape) > 1:\n",
    "        # Gather the starting values of each row of the mask.\n",
    "        # This will highlight all rows starting with pad tokens.\n",
    "        col = mask[:, 0]\n",
    "        # Apply the cumulative product to the isolated column.\n",
    "        # Since 1*1=1, 1*0=0, 0*0=0, the column is left with a sequence of 1's followed only by 0's.\n",
    "        # This will allow us to remove all values after first empty word.\n",
    "        col = tf.math.cumprod(col, axis=-1)\n",
    "        # Multiply this isolated column back over the mask.\n",
    "        mask = tf.multiply(mask, col[..., tf.newaxis])\n",
    "\n",
    "    # Apply row-wise denoising.\n",
    "    # Use cumulative product for the same reasoning as in column-wise denoising.\n",
    "    mask = tf.math.cumprod(mask, axis=-1)\n",
    "\n",
    "    # Multiply the (binary) mask back over the data.\n",
    "    result = tf.math.multiply(data, mask)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"CGAEL\", name=\"ArgmaxLayer\")\n",
    "class ArgmaxLayer(layer.Layer):\n",
    "    def __init__(self):\n",
    "        super(ArgmaxLayer, self).__init__()\n",
    "\n",
    "    def call(self, data, axis=-1):\n",
    "        return tf.math.argmax(data, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"CGAEL\", name=\"LanguageDenoiseLayer\")\n",
    "class LanguageDenoiseLayer(layer.Layer):\n",
    "    def __init__(self):\n",
    "        super(ArgmaxLayer, self).__init__()\n",
    "\n",
    "    def call(self, data):\n",
    "        return denoise_language(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
