{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append location to path to allow custom modules to be used.\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgael\n",
    "from cgael.models.SimpleColor import *\n",
    "from cgael.models.extras.LanguageDiscriminator import *\n",
    "from cgael.metrics import brevity\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layer\n",
    "\n",
    "import pygad\n",
    "import pygad.kerasga\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = os.path.join(\"C:\",os.sep,\"Users\",\"nicho\",\"PyProjects\",\"CGAEL_Results\")\n",
    "weights_file = os.path.join(root_folder, \"discriminator\", \"eng_L10_1.npy\")\n",
    "discrim = LanguageDiscriminatorModel(word_length=10, compile=False)\n",
    "discrim.model.set_weights(np.load(weights_file, allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = cgael.LanguageTokenSet(\"CHAT\", '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = ts.encode(\"CATCH THAT CAT\", (4, 10))\n",
    "msg2 = ts.encode(\"THAT CATH HAT\", (4, 10))\n",
    "msg3 = ts.encode(\"\", (4, 10))\n",
    "batch = tf.convert_to_tensor([msg1, msg2, msg3])\n",
    "tf.print(batch, summarize=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim.calculate_loss(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim.model(msg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reshape(batch, (-1, tf.shape(batch)[-1]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.equal(tf.math.mod(tf.range(tf.shape(a)[-2]), tf.shape(batch)[-2]), 0)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.math.not_equal(a[:,0], 0)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.math.logical_or(mask, temp)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = tf.boolean_mask(a, mask, axis=0)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = discrim(values)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = tf.reduce_mean(scores)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(discrim, data):\n",
    "    # Reshape tensor so that it is a list of all words.\n",
    "    # This is okay because they will all be averaged individually.\n",
    "    x = tf.reshape(data, (-1, tf.shape(data)[-1]))\n",
    "    # Remove all words that start with 0 from the list.\n",
    "    msk = tf.math.not_equal(x[:,0], 0)\n",
    "    x = tf.boolean_mask(x, msk, axis=0)\n",
    "    # Prevent errors.\n",
    "    if tf.equal(tf.size(x), 0):\n",
    "        return tf.constant(1.)\n",
    "    # Evaluate remaining words with discriminator.\n",
    "    print(x)\n",
    "    x = discrim(x)\n",
    "    # Calculate the mean and present value as loss.\n",
    "    return 1 - tf.reduce_mean(x)\n",
    "\n",
    "style_loss(discrim, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = cgael.LanguageTokenSet(\"CHAT\", '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set([\"A\", \"AT\", \"HA\", \"ACT\", \"CAT\", \"HAT\", \"CHAT\", \"TACT\", \"THAT\", \"CATCH\", \"HATCH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDiscriminatorGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, tokens, real_words:list, encode_length:int, batch_size:int, batch_count:int, len_min:int=1, len_max:int=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---\n",
    "        tokens : cgael.LanguageTokenSet\n",
    "            The language tokens to use for encoding and for the fake words.\n",
    "        real_words : list(String) OR set(String)\n",
    "            Words that you would like the discriminator to mark as real.\n",
    "            These words should be representative of the style of the language; consider excluding outliers.\n",
    "        encode_length : int\n",
    "            How long a word should be encoded as. Will have the shape (encode_length,).\n",
    "        batch_size : int\n",
    "            How big a batch is.\n",
    "        batch_count : int\n",
    "            How many batches per generation.\n",
    "        len_min : int\n",
    "            The minimum length for a fake word.\n",
    "            This value defaults to 1.\n",
    "        len_max : int\n",
    "            The maximum length for a fake word.\n",
    "            This value defaults to the value of encode_length.\n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        self.word_list = list(real_words) # For random values (set is not subscriptable).\n",
    "        self.word_set = set(real_words) # For a fast way to see if a word in real or not.\n",
    "        self.encode_shape = (encode_length,)\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_count = batch_count\n",
    "        self.len_min = len_min\n",
    "        self.len_max = encode_length if len_max is None else len_max\n",
    "        \n",
    "    def gibberish(self, length=None):\n",
    "        \"\"\"\n",
    "        Generates a random sequence of letters that may or may not match up to a real word.\n",
    "        \n",
    "        Parameters\n",
    "        ---\n",
    "        length : int\n",
    "            If supplied, it will generate a \"word\" of exactly that length.\n",
    "            Otherwise, it will generate a \"word\" between self.len_min and self.len_max in length.\n",
    "        \"\"\"\n",
    "        length = random.randint(self.len_min, self.len_max) if length is None else length\n",
    "        return ''.join(random.choices(self.tokens.alphabet_tokens, k=length))\n",
    "    \n",
    "    def nonsense(self, length=None):\n",
    "        \"\"\"\n",
    "        Generates a random sequence of letters that is never a real word (as provided).\n",
    "        \n",
    "        Parameters\n",
    "        ---\n",
    "        length : int\n",
    "            If supplied, it will generate a \"word\" of exactly that length.\n",
    "            Otherwise, it will generate a \"word\" between self.len_min and self.len_max in length.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            x = self.gibberish(length=length)\n",
    "            if x not in self.word_set:\n",
    "                return x\n",
    "            \n",
    "    # Required.\n",
    "    def __len__(self):\n",
    "        return self.batch_count\n",
    "\n",
    "    # Required.\n",
    "    def __getitem__(self, index=0):\n",
    "        #print(f\"[LanguageDiscriminatorGenerator.__getitem__] Called with index={index}.\")\n",
    "        \n",
    "        ls_x = []\n",
    "        ls_y = []\n",
    "        \n",
    "        def append_text(text, value):\n",
    "            ls_x.append(self.tokens.encode(text, shape=self.encode_shape))\n",
    "            ls_y.append(value)\n",
    "        \n",
    "        for _ in range(0, self.batch_size):\n",
    "            if random.random() < 0.5: # 50% chance of real value:\n",
    "                append_text(random.choice(self.word_list), 1)\n",
    "            else: # 50% chance of fake value:\n",
    "                append_text(self.nonsense(), 0)\n",
    "        \n",
    "        return tf.stack(ls_x), tf.stack(ls_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDiscriminatorModel():\n",
    "    def __init__(self, word_length, compile=True):\n",
    "        self.word_length = word_length\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        if compile:\n",
    "            self.model.compile(\n",
    "                loss = self._model_loss(),\n",
    "                optimizer = self._model_optimizer(),\n",
    "                metrics = self._model_metrics()\n",
    "            )\n",
    "        \n",
    "    def _build_model(self):\n",
    "        x = y = layer.Input((self.word_length,))\n",
    "        y = layer.Reshape((self.word_length, 1))(y)\n",
    "        y = layer.Conv1D(self.word_length, 5, padding=\"same\", activation=\"relu\")(y)\n",
    "        y = layer.Dense(1, activation=\"relu\")(y)\n",
    "        y = layer.Reshape((self.word_length,))(y)\n",
    "        y = layer.Dense(self.word_length, activation=\"relu\")(y)\n",
    "        y = layer.Dense(1, activation=\"sigmoid\")(y)\n",
    "        return keras.Model(x, y)\n",
    "        \n",
    "    def _model_loss(self):\n",
    "        return keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    \n",
    "    def _model_optimizer(self):\n",
    "        return keras.optimizers.Adam(0.001)\n",
    "    \n",
    "    def _model_metrics(self):\n",
    "        return [\"accuracy\"]\n",
    "    \n",
    "    def train(self, training_generator:LanguageDiscriminatorGenerator, epochs:int):\n",
    "        self.model.fit(training_generator, epochs=epochs)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        return self.model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = LanguageDiscriminatorGenerator(tokens=ts, real_words=english_words, encode_length=5, batch_size=16, batch_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = LanguageDiscriminatorModel(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim.train(gen, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"C:\",os.sep,\"Users\",\"nicho\",\"PyProjects\",\"CGAEL_Results\",\"demo.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(filepath, np.array(discrim.model.get_weights(), dtype=\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim.model(ts.encode(\"TA\", (1,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in english_words:\n",
    "    y = discrim.model(ts.encode(x, (1,5)))\n",
    "    print(x, y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    x = gen.nonsense()\n",
    "    y = discrim.model(ts.encode(x, (1,5)))\n",
    "    print(x, y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.__getitem__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = cgael.LanguageTokenSet(\"CHAT\", '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = ts.encode(\"CATCH THAT CAT\", shape=(4,5))\n",
    "msg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg2 = ts.encode(\"\", shape=(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg3 = ts.encode(\"CHAT AT HATCH HAT\", shape=(4,5))\n",
    "msg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg4 = ts.encode(\"ATATA TATAT ATATA TATAT\", shape=(4,5))\n",
    "msg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tf.convert_to_tensor([msg1, msg2, msg3, msg4])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgael.metrics import brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brevity.simple_brevity(msg1, power=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_brevity(data, word_length_power=2, word_count_power=2):\n",
    "    @tf.function\n",
    "    def helper(mask):\n",
    "        # STEP 1: WORD LENGTH POWER\n",
    "        # Get the lengths of each word.\n",
    "        x = tf.math.reduce_sum(mask, axis=-1)\n",
    "        # Divide by maximum length of words, placing the function on the range [0, 1].\n",
    "        x = tf.math.divide(x, data.shape[-1])\n",
    "        # Apply word_length_power.\n",
    "        x = tf.math.pow(x, word_length_power)\n",
    "        \n",
    "        # STEP 2: WORD COUNT POWER\n",
    "        # Get the sum of each word score.\n",
    "        x = tf.math.reduce_sum(x, axis=-1)\n",
    "        # Divide by maximum number of words, placing the function on the range [0, 1].\n",
    "        x = tf.math.divide(x, data.shape[-2])\n",
    "        # Apply word_count_power.\n",
    "        x = tf.math.pow(x, word_count_power)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    # Get binary mask of data.\n",
    "    mask = tf.sign(data)\n",
    "    sums = tf.math.reduce_sum(mask, axis=[-2, -1])\n",
    "    results = tf.where(\n",
    "        condition = tf.math.equal(sums, 0),\n",
    "        x = tf.constant(1, dtype=tf.float64),\n",
    "        y = helper(mask)\n",
    "    )\n",
    "    return tf.reduce_prod(results)\n",
    "    \n",
    "power_brevity(msg4, word_count_power=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = tf.math.reduce_sum(tf.math.sign(batch), axis=-1)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.math.reduce_sum(tf.math.reduce_sum(tf.math.sign(batch), axis=-1), axis=-1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = tf.math.reduce_sum(tf.math.sign(batch))\n",
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = tf.math.reduce_sum(tf.math.reduce_prod(batch.shape))\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = msg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.math.reduce_prod(tf.constant(batch.shape, dtype=tf.int64))\n",
    "e = tf.math.reduce_sum(tf.math.sign(batch), axis=[-2, -1])\n",
    "f = tf.math.equal(e, 0)\n",
    "g = batch.shape[-2]*batch.shape[-1]\n",
    "h = tf.where(f, g, tf.math.subtract(e,1))\n",
    "i = tf.math.reduce_sum(h)\n",
    "j = tf.math.divide(i, b)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_brevity(data):\n",
    "    # Get the number of non-padded tokens for each entry of the batch.\n",
    "    sums = tf.math.reduce_sum(tf.math.sign(data), axis=[-2, -1])\n",
    "    # Calculate the score of each entry of the batch 'n' such that:\n",
    "    # - if n == 0: maximum area of entry (worst possible score)\n",
    "    # - else: n - 1 (for calibration purposes)\n",
    "    # Remember: Golf rules; lower is better.\n",
    "    scores = tf.where(\n",
    "        condition = tf.math.equal(sums, 0),\n",
    "        x = tf.constant(data.shape[-2] * data.shape[-1], dtype=sums.dtype),\n",
    "        y = tf.math.subtract(sums, 1)\n",
    "    )\n",
    "    # Calculate the final loss by dividing sum of scores over maximum scores.\n",
    "    total = tf.math.reduce_sum(scores)\n",
    "    shape = tf.shape(data, out_type=sums.dtype)\n",
    "    area = tf.math.reduce_prod(shape)\n",
    "    loss = tf.math.divide(total, area)\n",
    "    return loss\n",
    "    \n",
    "simple_brevity(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = totals.numpy()\n",
    "print(a)\n",
    "b = area.numpy()\n",
    "print(b)\n",
    "c = batch.shape\n",
    "print(c)\n",
    "#d = batch.shape[-3]\n",
    "#print(d)\n",
    "d = [-1, -2]\n",
    "print(d)\n",
    "e = tf.math.reduce_sum(tf.math.sign(batch), axis=d)\n",
    "print(e)\n",
    "f = tf.math.equal(e, 0)\n",
    "print(f)\n",
    "g = batch.shape[-2]*batch.shape[-1]\n",
    "print(g)\n",
    "h = tf.where(f, g, tf.math.subtract(e,1))\n",
    "print(h)\n",
    "i = tf.math.reduce_sum(h)\n",
    "print(i)\n",
    "j = tf.math.divide(i, b)\n",
    "print(j)\n",
    "#k = tf.math.reduce_prod(tf.where())\n",
    "k = tf.math.mod(d, len(batch.shape))\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp(msg1, msg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.math.sign(msg)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(mask, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = cgael.LanguageTokenSet(\"CHAT\", '-')\n",
    "gen = SimpleColorGenerator([Swatch.WHITE, Swatch.BLACK, Swatch.RED, Swatch.GREEN, Swatch.BLUE, Swatch.YELLOW, Swatch.CYAN, Swatch.MAGENTA], blur=0, batch_lock=True)\n",
    "model = SimpleColorModel(token_set=ts, word_count=1, word_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_inst = model.train(\n",
    "    generator = gen,\n",
    "    generations = 100,\n",
    "    num_solutions = 50,\n",
    "    num_parents_mating = 5,\n",
    "    mutation_percent_genes = .2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(filepath, np.array(model.model.get_weights(), dtype=\"object\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ga_inst.plot_fitness(title=experiment_id, color=\"#0C69D3\")\n",
    "plot.savefig(os.path.join(root_folder, experiment_folder, f\"{experiment_id}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_SWATCH = \"swatch\"\n",
    "KEY_TEXT = \"text\"\n",
    "KEY_IN = \"input\"\n",
    "KEY_OUT = \"output\"\n",
    "KEY_ROUND = \"output_rounded\"\n",
    "\n",
    "swatches = [Swatch.BLACK, Swatch.RED, Swatch.GREEN, Swatch.YELLOW, Swatch.BLUE, Swatch.MAGENTA, Swatch.CYAN, Swatch.WHITE]\n",
    "samples = np.array([[sample_swatch(x)] for x in swatches])\n",
    "lang, out = model.model(samples)\n",
    "text = [ts.decode(x) for x in lang]\n",
    "\n",
    "d = {x:[] for x in [KEY_SWATCH, KEY_TEXT, KEY_IN, KEY_OUT, KEY_ROUND]}\n",
    "for s, t, i, o in zip(swatches, text, samples, out):\n",
    "    d[KEY_SWATCH].append(s)\n",
    "    d[KEY_TEXT].append(t)\n",
    "    d[KEY_IN].append(i)\n",
    "    o = o.numpy()\n",
    "    d[KEY_OUT].append(o)\n",
    "    o = np.round(o)\n",
    "    d[KEY_ROUND].append(o)\n",
    "df = pd.DataFrame(data=d)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_TEXT = \"text\"\n",
    "KEY_OUT = \"output\"\n",
    "KEY_ROUND = \"output_rounded\"\n",
    "\n",
    "def generate_words(tokens, n):\n",
    "    temp = tokens\n",
    "    total = [''] + temp\n",
    "    for _ in range(n-1):\n",
    "        temp = [x+y for x in temp for y in tokens]\n",
    "        total = total + temp\n",
    "    return total\n",
    "\n",
    "text = generate_words(ts.alphabet_tokens, 3)\n",
    "data = np.array([ts.encode(x, shape=(1,3)) for x in text])\n",
    "out = model.listener(data)\n",
    "\n",
    "d = {x:[] for x in [KEY_TEXT, KEY_OUT, KEY_ROUND]}\n",
    "for t, o in zip(text, out):\n",
    "    d[KEY_TEXT].append(t)\n",
    "    o = o.numpy()\n",
    "    d[KEY_OUT].append(o)\n",
    "    o = np.round(o)\n",
    "    d[KEY_ROUND].append(o)\n",
    "df = pd.DataFrame(data=d)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
