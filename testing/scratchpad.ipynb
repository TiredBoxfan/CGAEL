{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append location to path to allow custom modules to be used.\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nicho\\PyProjects\\CGAEL\\.conda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cgael\n",
    "from cgael.models.SimpleColor import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layer\n",
    "\n",
    "import pygad\n",
    "import pygad.kerasga\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = cgael.LanguageTokenSet(\"CHAT\", '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set([\"A\", \"AT\", \"HA\", \"ACT\", \"CAT\", \"HAT\", \"CHAT\", \"TACT\", \"THAT\", \"CATCH\", \"HATCH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDiscriminatorGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, tokens, real_words:list, encode_length:int, batch_size:int, batch_count:int, len_min:int=1, len_max:int=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---\n",
    "        tokens : cgael.LanguageTokenSet\n",
    "            The language tokens to use for encoding and for the fake words.\n",
    "        real_words : list(String) OR set(String)\n",
    "            Words that you would like the discriminator to mark as real.\n",
    "            These words should be representative of the style of the language; consider excluding outliers.\n",
    "        encode_length : int\n",
    "            How long a word should be encoded as. Will have the shape (encode_length,).\n",
    "        batch_size : int\n",
    "            How big a batch is.\n",
    "        batch_count : int\n",
    "            How many batches per generation.\n",
    "        len_min : int\n",
    "            The minimum length for a fake word.\n",
    "            This value defaults to 1.\n",
    "        len_max : int\n",
    "            The maximum length for a fake word.\n",
    "            This value defaults to the value of encode_length.\n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        self.word_list = list(real_words) # For random values (set is not subscriptable).\n",
    "        self.word_set = set(real_words) # For a fast way to see if a word in real or not.\n",
    "        self.encode_shape = (encode_length,)\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_count = batch_count\n",
    "        self.len_min = len_min\n",
    "        self.len_max = encode_length if len_max is None else len_max\n",
    "        \n",
    "    def gibberish(self, length=None):\n",
    "        \"\"\"\n",
    "        Generates a random sequence of letters that may or may not match up to a real word.\n",
    "        \n",
    "        Parameters\n",
    "        ---\n",
    "        length : int\n",
    "            If supplied, it will generate a \"word\" of exactly that length.\n",
    "            Otherwise, it will generate a \"word\" between self.len_min and self.len_max in length.\n",
    "        \"\"\"\n",
    "        length = random.randint(self.len_min, self.len_max) if length is None else length\n",
    "        return ''.join(random.choices(self.tokens.alphabet_tokens, k=length))\n",
    "    \n",
    "    def nonsense(self, length=None):\n",
    "        \"\"\"\n",
    "        Generates a random sequence of letters that is never a real word (as provided).\n",
    "        \n",
    "        Parameters\n",
    "        ---\n",
    "        length : int\n",
    "            If supplied, it will generate a \"word\" of exactly that length.\n",
    "            Otherwise, it will generate a \"word\" between self.len_min and self.len_max in length.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            x = self.gibberish(length=length)\n",
    "            if x not in self.word_set:\n",
    "                return x\n",
    "            \n",
    "    # Required.\n",
    "    def __len__(self):\n",
    "        return self.batch_count\n",
    "\n",
    "    # Required.\n",
    "    def __getitem__(self, index=0):\n",
    "        #print(f\"[LanguageDiscriminatorGenerator.__getitem__] Called with index={index}.\")\n",
    "        \n",
    "        ls_x = []\n",
    "        ls_y = []\n",
    "        \n",
    "        def append_text(text, value):\n",
    "            ls_x.append(self.tokens.encode(text, shape=self.encode_shape))\n",
    "            ls_y.append(value)\n",
    "        \n",
    "        for _ in range(0, self.batch_size):\n",
    "            if random.random() < 0.5: # 50% chance of real value:\n",
    "                append_text(random.choice(self.word_list), 1)\n",
    "            else: # 50% chance of fake value:\n",
    "                append_text(self.nonsense(), 0)\n",
    "        \n",
    "        return tf.stack(ls_x), tf.stack(ls_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDiscriminatorModel():\n",
    "    def __init__(self, word_length, compile=True):\n",
    "        self.word_length = word_length\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        if compile:\n",
    "            self.model.compile(\n",
    "                loss = self._model_loss(),\n",
    "                optimizer = self._model_optimizer(),\n",
    "                metrics = self._model_metrics()\n",
    "            )\n",
    "        \n",
    "    def _build_model(self):\n",
    "        x = y = layer.Input((self.word_length,))\n",
    "        y = layer.Reshape((self.word_length, 1))(y)\n",
    "        y = layer.Conv1D(self.word_length, 5, padding=\"same\", activation=\"relu\")(y)\n",
    "        y = layer.Dense(1, activation=\"relu\")(y)\n",
    "        y = layer.Reshape((self.word_length,))(y)\n",
    "        y = layer.Dense(self.word_length, activation=\"relu\")(y)\n",
    "        y = layer.Dense(1, activation=\"sigmoid\")(y)\n",
    "        return keras.Model(x, y)\n",
    "        \n",
    "    def _model_loss(self):\n",
    "        return keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    \n",
    "    def _model_optimizer(self):\n",
    "        return keras.optimizers.Adam(0.001)\n",
    "    \n",
    "    def _model_metrics(self):\n",
    "        return [\"accuracy\"]\n",
    "    \n",
    "    def train(self, training_generator:LanguageDiscriminatorGenerator, epochs:int):\n",
    "        self.model.fit(training_generator, epochs=epochs)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        return self.model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = LanguageDiscriminatorGenerator(tokens=ts, real_words=english_words, encode_length=5, batch_size=16, batch_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = LanguageDiscriminatorModel(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6934 - accuracy: 0.5031\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6926 - accuracy: 0.5525\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6910 - accuracy: 0.6356\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.6900\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6697 - accuracy: 0.7169\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6417 - accuracy: 0.7375\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6121 - accuracy: 0.7437\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5797 - accuracy: 0.7663\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5500 - accuracy: 0.7912\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5512 - accuracy: 0.7831\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5251 - accuracy: 0.8313\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5007 - accuracy: 0.8619\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4757 - accuracy: 0.8869\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4644 - accuracy: 0.8650\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4389 - accuracy: 0.8744\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4303 - accuracy: 0.8669\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3990 - accuracy: 0.8763\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3973 - accuracy: 0.8687\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.3641 - accuracy: 0.8925\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3607 - accuracy: 0.8881\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3705 - accuracy: 0.8838\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3563 - accuracy: 0.8819\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3463 - accuracy: 0.8712\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3274 - accuracy: 0.8875\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3149 - accuracy: 0.8944\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.3013 - accuracy: 0.8869\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.3088 - accuracy: 0.8875\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.2773 - accuracy: 0.9062\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.2841 - accuracy: 0.8894\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2721 - accuracy: 0.9075\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2723 - accuracy: 0.9306\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2702 - accuracy: 0.8969\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2870 - accuracy: 0.9006\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2567 - accuracy: 0.9244\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2452 - accuracy: 0.9444\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2436 - accuracy: 0.9294\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2210 - accuracy: 0.9406\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2066 - accuracy: 0.9375\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2253 - accuracy: 0.9337\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2112 - accuracy: 0.9388\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2149 - accuracy: 0.9425\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2088 - accuracy: 0.9494\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1862 - accuracy: 0.9469\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1916 - accuracy: 0.9419\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1877 - accuracy: 0.9556\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2087 - accuracy: 0.9544\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1685 - accuracy: 0.9675\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1772 - accuracy: 0.9431\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1770 - accuracy: 0.9506\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1789 - accuracy: 0.9525\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1651 - accuracy: 0.9631\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1738 - accuracy: 0.9594\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1628 - accuracy: 0.9513\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1673 - accuracy: 0.9475\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1451 - accuracy: 0.9588\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1697 - accuracy: 0.9519\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1505 - accuracy: 0.9675\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1468 - accuracy: 0.9581\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1541 - accuracy: 0.9606\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1468 - accuracy: 0.9644\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1546 - accuracy: 0.9694\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1403 - accuracy: 0.9719\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1406 - accuracy: 0.9656\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1462 - accuracy: 0.9644\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1375 - accuracy: 0.9681\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1274 - accuracy: 0.9712\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1488 - accuracy: 0.9581\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1369 - accuracy: 0.9644\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1290 - accuracy: 0.9756\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1406 - accuracy: 0.9669\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1218 - accuracy: 0.9750\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1300 - accuracy: 0.9669\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1148 - accuracy: 0.9694\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1201 - accuracy: 0.9694\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1102 - accuracy: 0.9737\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1127 - accuracy: 0.9694\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1122 - accuracy: 0.9712\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1091 - accuracy: 0.9719\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1004 - accuracy: 0.9781\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1040 - accuracy: 0.9769\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1077 - accuracy: 0.9756\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1166 - accuracy: 0.9719\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1017 - accuracy: 0.9787\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1096 - accuracy: 0.9719\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1103 - accuracy: 0.9750\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1027 - accuracy: 0.9787\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1061 - accuracy: 0.9762\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1058 - accuracy: 0.9762\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0955 - accuracy: 0.9825\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0965 - accuracy: 0.9794\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0906 - accuracy: 0.9819\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1070 - accuracy: 0.9744\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1010 - accuracy: 0.9762\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0920 - accuracy: 0.9825\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0982 - accuracy: 0.9794\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0827 - accuracy: 0.9837\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0830 - accuracy: 0.9812\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0865 - accuracy: 0.9837\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0852 - accuracy: 0.9837\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0898 - accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "discrim.train(gen, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 5)]               0         \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 5, 1)              0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 5, 5)              30        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5, 1)              6         \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72 (288.00 Byte)\n",
      "Trainable params: 72 (288.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discrim.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"C:\",os.sep,\"Users\",\"nicho\",\"PyProjects\",\"CGAEL_Results\",\"demo.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(filepath, np.array(discrim.model.get_weights(), dtype=\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.0581196]], dtype=float32)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrim.model(ts.encode(\"TA\", (1,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THAT [[0.9695126]]\n",
      "HA [[0.920142]]\n",
      "HAT [[0.95602405]]\n",
      "ACT [[0.9605899]]\n",
      "CAT [[0.95409435]]\n",
      "AT [[0.9265516]]\n",
      "CHAT [[0.9557561]]\n",
      "HATCH [[0.95602405]]\n",
      "TACT [[0.99135786]]\n",
      "A [[0.8553385]]\n",
      "CATCH [[0.94155306]]\n"
     ]
    }
   ],
   "source": [
    "for x in english_words:\n",
    "    y = discrim.model(ts.encode(x, (1,5)))\n",
    "    print(x, y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THATA [[0.00219612]]\n",
      "CC [[5.8126307e-07]]\n",
      "TA [[0.0581196]]\n",
      "TH [[0.00372074]]\n",
      "CTCTH [[1.6085067e-10]]\n",
      "TTH [[3.699416e-07]]\n",
      "HCHAA [[5.2133757e-12]]\n",
      "AHHHH [[9.36527e-13]]\n",
      "AHACC [[7.093687e-08]]\n",
      "TT [[0.7033022]]\n",
      "TCHC [[1.30813955e-08]]\n",
      "HATT [[0.01873811]]\n",
      "H [[2.7148458e-08]]\n",
      "ATCH [[0.00011253]]\n",
      "TAHHA [[2.0291404e-09]]\n",
      "HH [[0.00711142]]\n",
      "TAAHC [[2.6994752e-11]]\n",
      "ACACT [[1.697179e-05]]\n",
      "HHHTT [[2.9723565e-12]]\n",
      "HCHT [[0.00301502]]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    x = gen.nonsense()\n",
    "    y = discrim.model(ts.encode(x, (1,5)))\n",
    "    print(x, y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LanguageDiscriminatorGenerator.__getitem__] Called with index=0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16, 5), dtype=int64, numpy=\n",
       " array([[3, 0, 0, 0, 0],\n",
       "        [1, 3, 4, 1, 2],\n",
       "        [1, 3, 4, 1, 2],\n",
       "        [4, 3, 0, 0, 0],\n",
       "        [3, 1, 2, 1, 0],\n",
       "        [3, 2, 2, 3, 0],\n",
       "        [3, 1, 3, 3, 0],\n",
       "        [1, 3, 4, 3, 0],\n",
       "        [1, 3, 4, 1, 2],\n",
       "        [4, 1, 0, 0, 0],\n",
       "        [2, 3, 0, 0, 0],\n",
       "        [1, 4, 4, 4, 0],\n",
       "        [3, 4, 0, 0, 0],\n",
       "        [3, 4, 1, 0, 0],\n",
       "        [2, 3, 4, 1, 2],\n",
       "        [4, 0, 0, 0, 0]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(16,), dtype=int32, numpy=array([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.__getitem__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = cgael.LanguageTokenSet(\"CHAT\", '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = ts.encode(\"CATCH THAT CAT\", shape=(4,5))\n",
    "msg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg2 = ts.encode(\"\", shape=(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg3 = ts.encode(\"CHAT AT HATCH HAT\", shape=(4,5))\n",
    "msg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg4 = ts.encode(\"ATATA TATAT ATATA TATAT\", shape=(4,5))\n",
    "msg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tf.convert_to_tensor([msg1, msg2, msg3, msg4])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgael.metrics import brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brevity.simple_brevity(msg1, power=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_brevity(data, word_length_power=2, word_count_power=2):\n",
    "    @tf.function\n",
    "    def helper(mask):\n",
    "        # STEP 1: WORD LENGTH POWER\n",
    "        # Get the lengths of each word.\n",
    "        x = tf.math.reduce_sum(mask, axis=-1)\n",
    "        # Divide by maximum length of words, placing the function on the range [0, 1].\n",
    "        x = tf.math.divide(x, data.shape[-1])\n",
    "        # Apply word_length_power.\n",
    "        x = tf.math.pow(x, word_length_power)\n",
    "        \n",
    "        # STEP 2: WORD COUNT POWER\n",
    "        # Get the sum of each word score.\n",
    "        x = tf.math.reduce_sum(x, axis=-1)\n",
    "        # Divide by maximum number of words, placing the function on the range [0, 1].\n",
    "        x = tf.math.divide(x, data.shape[-2])\n",
    "        # Apply word_count_power.\n",
    "        x = tf.math.pow(x, word_count_power)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    # Get binary mask of data.\n",
    "    mask = tf.sign(data)\n",
    "    sums = tf.math.reduce_sum(mask, axis=[-2, -1])\n",
    "    results = tf.where(\n",
    "        condition = tf.math.equal(sums, 0),\n",
    "        x = tf.constant(1, dtype=tf.float64),\n",
    "        y = helper(mask)\n",
    "    )\n",
    "    return tf.reduce_prod(results)\n",
    "    \n",
    "power_brevity(msg4, word_count_power=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = tf.math.reduce_sum(tf.math.sign(batch), axis=-1)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.math.reduce_sum(tf.math.reduce_sum(tf.math.sign(batch), axis=-1), axis=-1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = tf.math.reduce_sum(tf.math.sign(batch))\n",
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = tf.math.reduce_sum(tf.math.reduce_prod(batch.shape))\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = msg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.math.reduce_prod(tf.constant(batch.shape, dtype=tf.int64))\n",
    "e = tf.math.reduce_sum(tf.math.sign(batch), axis=[-2, -1])\n",
    "f = tf.math.equal(e, 0)\n",
    "g = batch.shape[-2]*batch.shape[-1]\n",
    "h = tf.where(f, g, tf.math.subtract(e,1))\n",
    "i = tf.math.reduce_sum(h)\n",
    "j = tf.math.divide(i, b)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_brevity(data):\n",
    "    # Get the number of non-padded tokens for each entry of the batch.\n",
    "    sums = tf.math.reduce_sum(tf.math.sign(data), axis=[-2, -1])\n",
    "    # Calculate the score of each entry of the batch 'n' such that:\n",
    "    # - if n == 0: maximum area of entry (worst possible score)\n",
    "    # - else: n - 1 (for calibration purposes)\n",
    "    # Remember: Golf rules; lower is better.\n",
    "    scores = tf.where(\n",
    "        condition = tf.math.equal(sums, 0),\n",
    "        x = tf.constant(data.shape[-2] * data.shape[-1], dtype=sums.dtype),\n",
    "        y = tf.math.subtract(sums, 1)\n",
    "    )\n",
    "    # Calculate the final loss by dividing sum of scores over maximum scores.\n",
    "    total = tf.math.reduce_sum(scores)\n",
    "    shape = tf.shape(data, out_type=sums.dtype)\n",
    "    area = tf.math.reduce_prod(shape)\n",
    "    loss = tf.math.divide(total, area)\n",
    "    return loss\n",
    "    \n",
    "simple_brevity(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = totals.numpy()\n",
    "print(a)\n",
    "b = area.numpy()\n",
    "print(b)\n",
    "c = batch.shape\n",
    "print(c)\n",
    "#d = batch.shape[-3]\n",
    "#print(d)\n",
    "d = [-1, -2]\n",
    "print(d)\n",
    "e = tf.math.reduce_sum(tf.math.sign(batch), axis=d)\n",
    "print(e)\n",
    "f = tf.math.equal(e, 0)\n",
    "print(f)\n",
    "g = batch.shape[-2]*batch.shape[-1]\n",
    "print(g)\n",
    "h = tf.where(f, g, tf.math.subtract(e,1))\n",
    "print(h)\n",
    "i = tf.math.reduce_sum(h)\n",
    "print(i)\n",
    "j = tf.math.divide(i, b)\n",
    "print(j)\n",
    "#k = tf.math.reduce_prod(tf.where())\n",
    "k = tf.math.mod(d, len(batch.shape))\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp(msg1, msg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.math.sign(msg)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(mask, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = cgael.LanguageTokenSet(\"CHAT\", '-')\n",
    "gen = SimpleColorGenerator([Swatch.WHITE, Swatch.BLACK, Swatch.RED, Swatch.GREEN, Swatch.BLUE, Swatch.YELLOW, Swatch.CYAN, Swatch.MAGENTA], blur=0, batch_lock=True)\n",
    "model = SimpleColorModel(token_set=ts, word_count=1, word_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_inst = model.train(\n",
    "    generator = gen,\n",
    "    generations = 100,\n",
    "    num_solutions = 50,\n",
    "    num_parents_mating = 5,\n",
    "    mutation_percent_genes = .2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(filepath, np.array(model.model.get_weights(), dtype=\"object\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ga_inst.plot_fitness(title=experiment_id, color=\"#0C69D3\")\n",
    "plot.savefig(os.path.join(root_folder, experiment_folder, f\"{experiment_id}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_SWATCH = \"swatch\"\n",
    "KEY_TEXT = \"text\"\n",
    "KEY_IN = \"input\"\n",
    "KEY_OUT = \"output\"\n",
    "KEY_ROUND = \"output_rounded\"\n",
    "\n",
    "swatches = [Swatch.BLACK, Swatch.RED, Swatch.GREEN, Swatch.YELLOW, Swatch.BLUE, Swatch.MAGENTA, Swatch.CYAN, Swatch.WHITE]\n",
    "samples = np.array([[sample_swatch(x)] for x in swatches])\n",
    "lang, out = model.model(samples)\n",
    "text = [ts.decode(x) for x in lang]\n",
    "\n",
    "d = {x:[] for x in [KEY_SWATCH, KEY_TEXT, KEY_IN, KEY_OUT, KEY_ROUND]}\n",
    "for s, t, i, o in zip(swatches, text, samples, out):\n",
    "    d[KEY_SWATCH].append(s)\n",
    "    d[KEY_TEXT].append(t)\n",
    "    d[KEY_IN].append(i)\n",
    "    o = o.numpy()\n",
    "    d[KEY_OUT].append(o)\n",
    "    o = np.round(o)\n",
    "    d[KEY_ROUND].append(o)\n",
    "df = pd.DataFrame(data=d)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_TEXT = \"text\"\n",
    "KEY_OUT = \"output\"\n",
    "KEY_ROUND = \"output_rounded\"\n",
    "\n",
    "def generate_words(tokens, n):\n",
    "    temp = tokens\n",
    "    total = [''] + temp\n",
    "    for _ in range(n-1):\n",
    "        temp = [x+y for x in temp for y in tokens]\n",
    "        total = total + temp\n",
    "    return total\n",
    "\n",
    "text = generate_words(ts.alphabet_tokens, 3)\n",
    "data = np.array([ts.encode(x, shape=(1,3)) for x in text])\n",
    "out = model.listener(data)\n",
    "\n",
    "d = {x:[] for x in [KEY_TEXT, KEY_OUT, KEY_ROUND]}\n",
    "for t, o in zip(text, out):\n",
    "    d[KEY_TEXT].append(t)\n",
    "    o = o.numpy()\n",
    "    d[KEY_OUT].append(o)\n",
    "    o = np.round(o)\n",
    "    d[KEY_ROUND].append(o)\n",
    "df = pd.DataFrame(data=d)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
